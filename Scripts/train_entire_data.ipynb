{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# NLP Final - BART and BART-pubmed on eLife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6IzQOqgpu7H",
    "outputId": "a6ae3afe-977f-45b0-b55f-dd38e85d3159"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cpsc452_xc392/.conda/envs/cpsc552/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version is:  2.2.2\n",
      "You are using:  cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "# !pip install transformers datasets evaluate rouge_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "\n",
    "from typing import Tuple, List\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# We'll set the random seeds for deterministic results.\n",
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class Placeholder:\n",
    "    @property\n",
    "    def DO(self):\n",
    "        raise NotImplementedError(\"You haven't yet implemented this part of the assignment yet\")\n",
    "\n",
    "TO = Placeholder()\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Pytorch version is: \", torch.__version__)\n",
    "print(\"You are using: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/cpsc452_xc392/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import for model evaluation\n",
    "import os, sys, json\n",
    "import textstat\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score\n",
    "# from alignscore import AlignScore\n",
    "# from lens.lens_score import LENS\n",
    "import torch\n",
    "from summac.model_summac import SummaCConv\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IC-cQajVO0Jp",
    "outputId": "e4b25c8c-5aee-42da-9869-600bd5efdce0"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xSR_YHOjTce5"
   },
   "outputs": [],
   "source": [
    "# file_path = '/content/drive/My Drive/final_project/eLife_train.jsonl'\n",
    "file_path = 'eLife_train.jsonl'\n",
    "file_path_test = 'eLife_val.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gY5DZJWiTiH0",
    "outputId": "199c30f5-2397-44ac-f5ef-9dcd90bcbfa5"
   },
   "outputs": [],
   "source": [
    "# load data by keyword\n",
    "# load entire dataset: keyword = None\n",
    "def keyword_data(file_path,keyword=None):\n",
    "    rows_list = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            json_data = json.loads(line.strip())\n",
    "\n",
    "            if not keyword:\n",
    "                rows_list.append(json_data)\n",
    "            if 'keywords' in json_data and keyword in json_data['keywords']:\n",
    "                rows_list.append(json_data)\n",
    "    return rows_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rouge(preds, refs):\n",
    "  # Get ROUGE F1 scores\n",
    "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], \\\n",
    "                                    use_stemmer=True, split_summaries=True)\n",
    "  scores = [scorer.score(p, refs[i]) for i, p in enumerate(preds)]\n",
    "  return np.mean([s['rouge1'].fmeasure for s in scores]), \\\n",
    "         np.mean([s['rouge2'].fmeasure for s in scores]), \\\n",
    "         np.mean([s['rougeLsum'].fmeasure for s in scores])\n",
    "\n",
    "def calc_bertscore(preds, refs):\n",
    "  # Get BERTScore F1 scores\n",
    "  P, R, F1 = score(preds, refs, lang=\"en\", verbose=True, device='cuda:0')\n",
    "  return np.mean(F1.tolist())\n",
    "\n",
    "def calc_readability(preds):\n",
    "  fkgl_scores = []\n",
    "  cli_scores = []\n",
    "  dcrs_scores = []\n",
    "  for pred in preds:\n",
    "    fkgl_scores.append(textstat.flesch_kincaid_grade(pred))\n",
    "    cli_scores.append(textstat.coleman_liau_index(pred))\n",
    "    dcrs_scores.append(textstat.dale_chall_readability_score(pred))\n",
    "  return np.mean(fkgl_scores), np.mean(cli_scores), np.mean(dcrs_scores)\n",
    "\n",
    "def calc_lens(preds, refs, docs):\n",
    "  model_path = \"./models/LENS/LENS/checkpoints/epoch=5-step=6102.ckpt\"\n",
    "  metric = LENS(model_path, rescale=True)\n",
    "  abstracts = [d.split(\"\\n\")[0] for d in docs]\n",
    "  refs = [[x] for x in refs]\n",
    "\n",
    "  scores = metric.score(abstracts, preds, refs, batch_size=8, gpus=1)\n",
    "  return np.mean(scores)\n",
    "\n",
    "def calc_alignscore(preds, docs):\n",
    "  alignscorer = AlignScore(model='roberta-base', batch_size=16, device='cuda:0', \\\n",
    "                           ckpt_path='./models/AlignScore/AlignScore-base.ckpt', evaluation_mode='nli_sp')\n",
    "  return np.mean(alignscorer.score(contexts=docs, claims=preds))\n",
    "\n",
    "def cal_summac(preds, docs):\n",
    "  model_conv = SummaCConv(models=[\"vitc\"], bins='percentile', granularity=\"sentence\", nli_labels=\"e\", device=\"cuda\", start_file=\"default\", agg=\"mean\")\n",
    "  return np.mean(model_conv.score(docs, preds)['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cpsc452_xc392/.conda/envs/cpsc552/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/cpsc452_xc392/.conda/envs/cpsc552/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "model_name = \"facebook/bart-large-xsum\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)  # load the tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(DEVICE)  # load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'eLife_train.jsonl'\n",
    "file_path_test = 'eLife_val.jsonl'\n",
    "\n",
    "eLife_data=pd.DataFrame(keyword_data(file_path))\n",
    "eLife_data_val=pd.DataFrame(keyword_data(file_path_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rjLjxadov--V"
   },
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(dataframe):\n",
    "    # first 512 tokens\n",
    "    dataframe['input_text'] = dataframe['article'].apply(lambda x: ' '.join(x.split()[:512]))\n",
    "    # tokenization\n",
    "    inputs = tokenizer(dataframe['input_text'].tolist(), max_length=512, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    # tokenize output\n",
    "    outputs = tokenizer(dataframe['lay_summary'].tolist(), max_length=128, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    return inputs, outputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_labels = preprocess_and_tokenize(eLife_data)\n",
    "val_inputs, val_labels = preprocess_and_tokenize(eLife_data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0-xT783ZwLK8"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MedicineDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# # Dataset\n",
    "train_dataset = MedicineDataset(train_inputs, train_labels)\n",
    "val_dataset = MedicineDataset(val_inputs, val_labels)\n",
    "\n",
    "# DataLoader\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YrKcNXkP1asY"
   },
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "from rouge_score import rouge_scorer\n",
    "import textstat\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    predictions = p.predictions\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    predicted_ids = predictions.argmax(-1)\n",
    "\n",
    "    decoded_preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in predicted_ids]\n",
    "    decoded_labels = [tokenizer.decode(l, skip_special_tokens=True, clean_up_tokenization_spaces=True) for l in p.label_ids]\n",
    "\n",
    "    # Relevance scores\n",
    "    rouge_results = calc_rouge(decoded_preds, decoded_labels)\n",
    "    rouge_results = {key: results for key, results in zip(['rouge1' , 'rouge2', 'rougeL'], list(rouge_results))}\n",
    "    bert_score = calc_bertscore(decoded_preds, decoded_labels)\n",
    "    \n",
    "    # Readability scores\n",
    "    avg_fkgl, avg_cli, avg_dcrs = calc_readability(decoded_preds)\n",
    "\n",
    "    return {\n",
    "        **rouge_results,\n",
    "        \"bert_score\": bert_score,\n",
    "        \"avg_fkgl\": avg_fkgl,\n",
    "        \"avg_cli\": avg_cli,\n",
    "        \"avg_dcrs\": avg_dcrs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "OGm7VwLBwQ9t",
    "outputId": "44e898c5-3f5c-4bdd-a783-0b98abd7b3d7"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              # Training epochs\n",
    "    per_device_train_batch_size=8,   \n",
    "    per_device_eval_batch_size=8,    \n",
    "    warmup_steps=100,                \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,                \n",
    "    evaluation_strategy=\"steps\",     \n",
    "    eval_steps=100,                  \n",
    "    save_strategy=\"steps\",           \n",
    "    save_steps=500,                  # Enable mixed precision training\n",
    "    fp16=True,                        \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_save_path = \"project/LaySumProject/results\"\n",
    "keyword = \"bart\"\n",
    "keyword_save_path = os.path.join(base_model_save_path, f\"model_save_{keyword.replace(' ', '_')}\")\n",
    "os.makedirs(keyword_save_path, exist_ok=True)\n",
    "model.save_pretrained(keyword_save_path)\n",
    "tokenizer.save_pretrained(keyword_save_path)\n",
    "\n",
    "predict_output = trainer.predict(val_dataset)\n",
    "metrics = compute_metrics(predict_output)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "Go6RoeSt0MVH",
    "outputId": "165a546f-0c75-495c-ca49-2aaa98714a43"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da0a349d68a42209e63906857028b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac948714ed34f4593d49e57c498f7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.87 seconds, 84.07 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.264357089996338,\n",
       " 'eval_rouge1': 0.5782105879938336,\n",
       " 'eval_rouge2': 0.22440689302668687,\n",
       " 'eval_rougeL': 0.5416079381523048,\n",
       " 'eval_bert_score': 0.8693327740514921,\n",
       " 'eval_avg_fkgl': 9.169294605809128,\n",
       " 'eval_avg_cli': 10.619585062240665,\n",
       " 'eval_avg_dcrs': 8.974730290456431,\n",
       " 'eval_runtime': 9.6736,\n",
       " 'eval_samples_per_second': 24.913,\n",
       " 'eval_steps_per_second': 3.205,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART-pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model BART pubmed\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mse30/bart-base-finetuned-pubmed\")\n",
    "model_pubmed = AutoModelForSeq2SeqLM.from_pretrained(\"mse30/bart-base-finetuned-pubmed\")\n",
    "\n",
    "file_path = 'eLife_train.jsonl'\n",
    "file_path_test = 'eLife_val.jsonl'\n",
    "\n",
    "eLife_data=pd.DataFrame(keyword_data(file_path))\n",
    "eLife_data_val=pd.DataFrame(keyword_data(file_path_test))\n",
    "\n",
    "train_inputs, train_labels = preprocess_and_tokenize(eLife_data)\n",
    "val_inputs, val_labels = preprocess_and_tokenize(eLife_data_val)\n",
    "\n",
    "train_dataset = MedicineDataset(train_inputs, train_labels)\n",
    "val_dataset = MedicineDataset(val_inputs, val_labels)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/cpsc452_xc392/.conda/envs/cpsc552/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1632' max='1632' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1632/1632 04:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Bert Score</th>\n",
       "      <th>Avg Fkgl</th>\n",
       "      <th>Avg Cli</th>\n",
       "      <th>Avg Dcrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.042800</td>\n",
       "      <td>2.845975</td>\n",
       "      <td>0.510394</td>\n",
       "      <td>0.154672</td>\n",
       "      <td>0.475180</td>\n",
       "      <td>0.848120</td>\n",
       "      <td>7.221992</td>\n",
       "      <td>8.931950</td>\n",
       "      <td>8.201826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.928600</td>\n",
       "      <td>2.697470</td>\n",
       "      <td>0.519499</td>\n",
       "      <td>0.164124</td>\n",
       "      <td>0.480498</td>\n",
       "      <td>0.852327</td>\n",
       "      <td>8.526141</td>\n",
       "      <td>9.681826</td>\n",
       "      <td>8.457676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.822900</td>\n",
       "      <td>2.638843</td>\n",
       "      <td>0.526143</td>\n",
       "      <td>0.170059</td>\n",
       "      <td>0.488964</td>\n",
       "      <td>0.853601</td>\n",
       "      <td>8.103734</td>\n",
       "      <td>9.820913</td>\n",
       "      <td>8.483071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.879900</td>\n",
       "      <td>2.579447</td>\n",
       "      <td>0.535481</td>\n",
       "      <td>0.173346</td>\n",
       "      <td>0.495976</td>\n",
       "      <td>0.855289</td>\n",
       "      <td>8.454772</td>\n",
       "      <td>9.724564</td>\n",
       "      <td>8.449087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.761000</td>\n",
       "      <td>2.554208</td>\n",
       "      <td>0.536934</td>\n",
       "      <td>0.177310</td>\n",
       "      <td>0.501212</td>\n",
       "      <td>0.856432</td>\n",
       "      <td>7.692531</td>\n",
       "      <td>9.663154</td>\n",
       "      <td>8.420539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.623000</td>\n",
       "      <td>2.540445</td>\n",
       "      <td>0.538741</td>\n",
       "      <td>0.181090</td>\n",
       "      <td>0.500014</td>\n",
       "      <td>0.856913</td>\n",
       "      <td>8.184647</td>\n",
       "      <td>9.455975</td>\n",
       "      <td>8.498589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.559600</td>\n",
       "      <td>2.521262</td>\n",
       "      <td>0.542475</td>\n",
       "      <td>0.183624</td>\n",
       "      <td>0.500809</td>\n",
       "      <td>0.857806</td>\n",
       "      <td>9.042739</td>\n",
       "      <td>9.775021</td>\n",
       "      <td>8.677552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.577600</td>\n",
       "      <td>2.505866</td>\n",
       "      <td>0.546267</td>\n",
       "      <td>0.183392</td>\n",
       "      <td>0.506860</td>\n",
       "      <td>0.858679</td>\n",
       "      <td>8.436515</td>\n",
       "      <td>9.975685</td>\n",
       "      <td>8.640456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.559100</td>\n",
       "      <td>2.500283</td>\n",
       "      <td>0.543899</td>\n",
       "      <td>0.185821</td>\n",
       "      <td>0.504478</td>\n",
       "      <td>0.858701</td>\n",
       "      <td>8.433610</td>\n",
       "      <td>9.813859</td>\n",
       "      <td>8.542739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.421100</td>\n",
       "      <td>2.490222</td>\n",
       "      <td>0.543913</td>\n",
       "      <td>0.186455</td>\n",
       "      <td>0.506282</td>\n",
       "      <td>0.858994</td>\n",
       "      <td>8.655602</td>\n",
       "      <td>10.080996</td>\n",
       "      <td>8.649212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.390700</td>\n",
       "      <td>2.481130</td>\n",
       "      <td>0.547378</td>\n",
       "      <td>0.187779</td>\n",
       "      <td>0.507220</td>\n",
       "      <td>0.860371</td>\n",
       "      <td>8.575104</td>\n",
       "      <td>10.070124</td>\n",
       "      <td>8.723568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.396700</td>\n",
       "      <td>2.473928</td>\n",
       "      <td>0.548147</td>\n",
       "      <td>0.188088</td>\n",
       "      <td>0.508493</td>\n",
       "      <td>0.860251</td>\n",
       "      <td>9.030290</td>\n",
       "      <td>10.370207</td>\n",
       "      <td>8.723278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.411800</td>\n",
       "      <td>2.469625</td>\n",
       "      <td>0.547648</td>\n",
       "      <td>0.190620</td>\n",
       "      <td>0.509257</td>\n",
       "      <td>0.860372</td>\n",
       "      <td>8.691286</td>\n",
       "      <td>10.196846</td>\n",
       "      <td>8.717012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.355800</td>\n",
       "      <td>2.467906</td>\n",
       "      <td>0.547310</td>\n",
       "      <td>0.189185</td>\n",
       "      <td>0.510342</td>\n",
       "      <td>0.860198</td>\n",
       "      <td>8.391701</td>\n",
       "      <td>10.160249</td>\n",
       "      <td>8.661494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.360300</td>\n",
       "      <td>2.463868</td>\n",
       "      <td>0.547895</td>\n",
       "      <td>0.190026</td>\n",
       "      <td>0.509664</td>\n",
       "      <td>0.860815</td>\n",
       "      <td>8.562241</td>\n",
       "      <td>10.108755</td>\n",
       "      <td>8.716556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.401900</td>\n",
       "      <td>2.458894</td>\n",
       "      <td>0.549155</td>\n",
       "      <td>0.191496</td>\n",
       "      <td>0.511089</td>\n",
       "      <td>0.860777</td>\n",
       "      <td>8.605809</td>\n",
       "      <td>10.103734</td>\n",
       "      <td>8.700913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9caabce515a54667a7c083d3c110e38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0074a831d4cf41e188096c86f7b6a7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.89 seconds, 83.30 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af12dda54ef4439bce9f38d48d6ea4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0291651040564ab7b50cea18910aa89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.89 seconds, 83.28 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be16c931e4fa4d9eb026257aa14e1c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0876dbbfd1ef4917a3ec7b1ec6c7c6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.93 seconds, 82.38 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef528f1e234b4bfa940b183038fa69f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aad1e068a7c49ea94d366d85f4bc20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.92 seconds, 82.42 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84e1f9e877c42e6a823c6e094be33b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4a7ed9fe2240ec91ee022ba4780b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.91 seconds, 82.80 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8e6fc6f3464288b92445392775bc90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85227522cfcc4b4ea34f902203f564f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.90 seconds, 83.03 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6013bb7610a8484489860e6d5fd32e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb3e64fe7114c2d96c60936ff17bea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.89 seconds, 83.41 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43a75394db8496cbdecef82032a2a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d9d77bc0474227909d2a1098514bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.90 seconds, 83.22 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b043c7e54f654e9baab7bad0edaf5507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1af2213a304186ada0450d6ea613db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.91 seconds, 82.81 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e2f395e5fa46b99848c4abc7e0e7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7348da304f4cbf867f41302504cad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.91 seconds, 82.71 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd79cf89efc49aab1c66c182eb01a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c73a86436e4fce89fd4a99084e69e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.91 seconds, 82.75 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd56508a212f460dacfa6df341202c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fb1a64f4e045ecad5b16537d3b7f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.87 seconds, 83.94 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f32e692812d4e3393131036559b33b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b15650998d4ecda02a9e04eec81692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.93 seconds, 82.36 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4baaecca6c4daaad37740959ea6bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504b31739de24fafa16986ae5c5c5ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.89 seconds, 83.33 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c011645ac6345b398aec3e5d56fd375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2915d39426654a549891a32db9d6cbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.90 seconds, 83.11 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ccca892c5c448d9822bba70d87008d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a74bce23b1c416b882850e3ab308d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.91 seconds, 82.71 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1632, training_loss=2.6181880703159406, metrics={'train_runtime': 266.2892, 'train_samples_per_second': 48.962, 'train_steps_per_second': 6.129, 'total_flos': 3974871971266560.0, 'train_loss': 2.6181880703159406, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=8,   \n",
    "    per_device_eval_batch_size=8,    \n",
    "    warmup_steps=100,                \n",
    "    weight_decay=0.01,               \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,                \n",
    "    evaluation_strategy=\"steps\",     \n",
    "    eval_steps=100,                  \n",
    "    save_strategy=\"steps\",           \n",
    "    save_steps=500,                  \n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_pubmed,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_save_path = \"project/LaySumProject/results\"\n",
    "keyword = \"bartpubmed\"\n",
    "keyword_save_path = os.path.join(base_model_save_path, f\"model_save_{keyword.replace(' ', '_')}\")\n",
    "os.makedirs(keyword_save_path, exist_ok=True)\n",
    "model.save_pretrained(keyword_save_path)\n",
    "tokenizer.save_pretrained(keyword_save_path)\n",
    "\n",
    "predict_output = trainer.predict(val_dataset)\n",
    "metrics = compute_metrics(predict_output, trainer.docs)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7daab3b54447ef9595865cab40fbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4416515f4762430ab48a51f8864d7813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.89 seconds, 83.28 sentences/sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.4586825370788574,\n",
       " 'eval_rouge1': 0.5492260573721498,\n",
       " 'eval_rouge2': 0.19132091134194984,\n",
       " 'eval_rougeL': 0.5106251910564675,\n",
       " 'eval_bert_score': 0.8607087429628333,\n",
       " 'eval_avg_fkgl': 8.707053941908715,\n",
       " 'eval_avg_cli': 10.129626556016596,\n",
       " 'eval_avg_dcrs': 8.711784232365146,\n",
       " 'eval_runtime': 7.5095,\n",
       " 'eval_samples_per_second': 32.093,\n",
       " 'eval_steps_per_second': 4.128,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cpsc552",
   "language": "python",
   "name": "cpsc552"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
